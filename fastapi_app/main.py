from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
import os
from dotenv import load_dotenv
from openai import OpenAI
import logging
import time

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load API Key t·ª´ .env
load_dotenv()
api_key = os.getenv("KEY_API_GPT")

if not api_key:
    logger.error("‚ùå API Key kh√¥ng t√¨m th·∫•y! Ki·ªÉm tra l·∫°i .env")
    raise ValueError("‚ùå API Key kh√¥ng t√¨m th·∫•y! Ki·ªÉm tra l·∫°i .env")

# T·∫°o client OpenAI
try:
    client = OpenAI(api_key=api_key)
    logger.info("‚úÖ ƒê√£ kh·ªüi t·∫°o OpenAI client")
except Exception as e:
    logger.error(f"‚ùå L·ªói khi kh·ªüi t·∫°o OpenAI client: {str(e)}")
    raise

app = FastAPI()

# C·∫•u h√¨nh CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://127.0.0.1:8001",
        "http://localhost:8001",
        "http://127.0.0.1:8000",  # Th√™m n·∫øu Laravel ch·∫°y tr√™n c·ªïng kh√°c
        "http://localhost:8000",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Model nh·∫≠n d·ªØ li·ªáu
class ChatRequest(BaseModel):
    message: str

@app.get("/")
async def read_root():
    return {"message": "‚úÖ API Chatbot Y H·ªçc GPT-4o ƒëang ch·∫°y!"}

@app.post("/chatbot")
async def chatbot_response(request: ChatRequest):
    user_message = request.message.strip()

    if not user_message:
        logger.warning("Tin nh·∫Øn r·ªóng t·ª´ client")
        raise HTTPException(status_code=400, detail="‚ùå Tin nh·∫Øn kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng!")

    try:
        logger.info(f"Nh·∫≠n tin nh·∫Øn: {user_message}")

        # X·ª≠ l√Ω logic cho tin nh·∫Øn VIP
        is_vip = "Tr·∫£ l·ªùi chuy√™n s√¢u" in user_message
        system_prompt = (
            "B·∫°n l√† m·ªôt b√°c sƒ© AI chuy√™n v·ªÅ y h·ªçc. H√£y tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn s·ª©c kh·ªèe m·ªôt c√°ch ch√≠nh x√°c, d·ªÖ hi·ªÉu, v√† cung c·∫•p l·ªùi khuy√™n h·ªØu √≠ch."
        )
        if is_vip:
            system_prompt += (
                " H√£y cung c·∫•p c√¢u tr·∫£ l·ªùi chi ti·∫øt, c√≥ d·∫´n ch·ª©ng y khoa n·∫øu c·∫ßn, v√† tr√¨nh b√†y r√µ r√†ng."
            )

        # Th√™m retry logic cho OpenAI
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_message}
                    ],
                    temperature=0.7,
                    max_tokens=1500 if is_vip else 1000,  # TƒÉng tokens cho VIP
                    timeout=30,  # Timeout cho m·ªói y√™u c·∫ßu
                )
                bot_reply = response.choices[0].message.content.strip()
                logger.info("G·ª≠i ph·∫£n h·ªìi th√†nh c√¥ng")
                return {"response": bot_reply}
            except Exception as e:
                logger.warning(f"Th·ª≠ l·∫ßn {attempt + 1} th·∫•t b·∫°i: {str(e)}")
                if attempt == max_retries - 1:
                    raise
                time.sleep(2 ** attempt)  # Exponential backoff

    except Exception as e:
        logger.error(f"üö® L·ªói khi g·ªçi OpenAI: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"üö® Kh√¥ng th·ªÉ x·ª≠ l√Ω y√™u c·∫ßu do l·ªói t·ª´ h·ªá th·ªëng AI: {str(e)}"
        )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

